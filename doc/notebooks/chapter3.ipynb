{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "การควบคุมเหมาะที่สุดและการเรียนรูู้เสริมกำลัง -- ดร.วโรดม ตู้จินดา"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 การควบคุมเหมาะที่สุด\n",
    "\n",
    "หลังจากที่ได้ศึกษาเกี่ยวกับพื้นฐานการหาค่าเหมาะที่สุดในบทที่ 2 แล้ว ในบทนี้จะเริ่มต้นเข้าสู่การประุยุกต์ใช้งานในสาขาระบบควบคุม \n",
    "ซึ่งเป็นสาระสำคัญของหนังสือนี้ ดังที่ได้กล่าวแนะนำในบทแรกแล้วว่า \n",
    "แนวทางในการออกแบบและสร้างตัวควบคุมสมัยใหม่แตกต่างจากเดิมที่ใช้ตัวควบคุมที่มีโครงสร้างแน่นอน \n",
    "เช่นอยู่ในรูปของการป้อนกลับสถานะหรือเอาต์พุตผ่านอัตราขยายหรือสัมประสิทธิ์ฟังก์ชันถ่ายโอนที่เป็นค่าคงที่ \n",
    "โดยตัวควบคุมผ่านการออกแบบโดยวิธีการต่างๆ หรือหากเป็นตัวควบคุม PID \n",
    "จะอาศัยการปรับแต่งค่าพารามิเตอร์แบบออฟไลน์โดยผู้ใช้งานเพื่อได้ผลตอบสนองตามต้องการ \n",
    "การควบคุมปรับตัว (adaptive control) จะมีความยืดหยุ่นเพิ่มขึ้นโดยสามารถปรับค่าพารามิเตอร์อัตโนมัติในขณะทำงาน \n",
    "กล่าวได้ว่าเป็นการเรียนรู้ในรูปแบบหนึ่ง \n",
    "โดยบางวิธีการอาจมีความใกล้เคียงกับแนวทางที่นำเสนอในหนังสือนี้แต่อาจไม่ได้ถูกจัดรูปให้มองเห็นความสัมพันธ์อย่างชัดจน \n",
    "\n",
    "แท้จริงแล้วแนวทางการควบคุมเหมาะที่สุดมิได้เกิดขึ้นในยุคปัจจุบัน แต่เริ่มต้นตั้งแต่ในสมัยทศวรรษที่ 60 \n",
    "ของคริสต์ศักราช  เมื่อเริ่มมีการใช้คอมพิวเตอร์ช่วยในการวิเคราะห์และออกแบบระบบควบคุมในรูปปริภูมิสถานะ \n",
    "โดยการควบคุมป้อนกลับสถานะที่เรียกว่า ตัวควบคุมกำลังสองเชิงเส้น (linear quadratic regulator) \n",
    "ซึ่งต่อไปจะเรียกชื่อย่อว่า LQR ตัวกรองคาลมาน (Kalman filter หรือ KF) หรือรวมกันเรียกว่า \n",
    "ตัวควบคุมกำลังสองแบบเกาส์เซียน (linear quadratic gaussian หรือ LQG) ล้วนมีพื้นฐานบนวิธีการหาค่าเหมาะที่สุด \n",
    "เพียงแต่สมรรถนะของคอมพิวเตอร์ในสมัยนั้นไม่เพียงพอที่จะแก้ปัญหาเหมาะที่สุดแบบเรียลไทม์ ดังนั้น LQR, KF, LQG \n",
    "จะถูกสังเคราะห์แบบออฟไลน์เป็นอัตราขยายหรือฟังก์ชันถ่ายโอนคงที่ และอิมพลิเมนต์เป็นตัวควบคุมป้อนกลับที่ไม่มีการปรับแต่ง \n",
    "ในขณะใช้งาน ในขณะที่การแก้ปัญหาเหมาะที่สุดแบบออนไลน์ในการควบคุมแบบทำนายโมเดล (model predictive control หรือ MPC) \n",
    "มีการใช้งานจำกัดเฉพาะในระบบที่ตอบสนองช้าเช่นกระบวนการทางเคมี เนื่องจากการหาคำตอบแบบออนไลน์ต้องใช้เวลานาน \n",
    "\n",
    "## 3.1 การควบคุมเหมาะที่สุดแบบเชิงกำหนด\n",
    "\n",
    "ประเด็นสำคัญที่แตกต่างระหว่างตัวควบคุมเหมาะที่สุดกับตัวควบคุมที่อาศัยการปรับแต่งพารามิเตอร์เช่น PID คือ \n",
    "ในการควบคุมเหมาะที่สุดเราจะกำหนดดรรชนีที่เป็นตัวชี้วัดว่าตัวควบคุมทำงานได้ดีมากน้อยเพียงใด เช่น \n",
    "ความแตกต่างในการตามรอยแนววิถีที่กำหนด ความเร็วในการทำงาน หรือดรรชนีอื่นตามต้องการ \n",
    "\n",
    "เริ่มต้นจากปัญหาการควบคุมเหมาะที่สุดแบบเชิงกำหนด (deterministic) \n",
    "ที่การหาคำตอบจะคงเดิมโดยไม่คำนึงถึงความไม่แน่นอนในตัวแปรเช่นสัญญาณรบกวน \n",
    "ตัวอย่างของโจทย์ปัญหาเป็นดังนี้ \n",
    "\n",
    "$$\n",
    "\\underset{x(t),u(t)}{min} \\; J(x(t),u(t))  \n",
    "$$\n",
    "$$\n",
    "s.t. \\;\\; \\dot{x}(t) = f(x(t),u(t)),\n",
    "$$\n",
    "$$\n",
    "\\;\\; U_{min} \\le u(t) \\le U_{max}, \n",
    "$$\n",
    "$$\n",
    "\\;\\; X_{min} \\le x(t) \\le X_{max} \\tag{3.1}\n",
    "$$\n",
    "จะเห็นว่าประกอบด้วยฟังก์ชันมูลค่าซึ่งเป็นตัวชี้วัดสมรรถนะ \n",
    "เพื่อคำนวณหาเอาต์พุตตัวควบคุมที่ทำให้ฟังก์ชันมูลค่านั้นน้อยที่สุด ใช้เอาต์พุตนั้นเป็นตัวขับเคลื่อนพลานต์ \n",
    "คือระบบที่ต้องการควบคุม ดังนั้นพลวัตของพลานต์เป็นเงื่อนไขบังคับสำคัญสำหรับปัญหาการควบคุมเหมาะที่สุด \n",
    "ซึ่งจะอยู่ในรูปเงื่อนไขบังคับสมการ นอกจากนั้นอาจมีเงื่อนไขบังคับอสมการเพิ่มเติม เช่นการจำกัดค่าของเอาต์พุตควบคุม \n",
    "หรือการจำกัดค่าสถานะหรือเอาต์พุตเพิ่อมิให้ชนกับสิ่งกีดขวาง เป็นต้น \n",
    "\n",
    "สำหรับผู้อ่านที่คุ้นเคยกับปัญหาการเรียนรู้เสริมกำลัง (ต่อไปจะเรียกย่อว่า RL)  ประเด็นสำคัญและคำศัพท์ที่แตกต่างพอสรุปได้คือ\n",
    "\n",
    "* ฟังก์ชันวัตถุประสงค์ในปัญหา RL อยู่ในรูปของรางวัล โดยต้องการหาค่าสูงสุดของรางวัลที่ได้รับ\n",
    "* เอาต์พุตของตัวควบคุมถูกเรียกว่าการกระทำ (action) หรือนโยบาย (policy)\n",
    "* พลานต์หรือพลวัตที่ต้องการควบคุมถูกเรียกว่าสภาพแวดล้อม (environment)\n",
    "* ใช้คำว่า ตัวกระทำ (agent) แทนตัวควบคุม\n",
    "* การเรียนรู้ในปัญหา RL ส่วนใหญ๋เป็นแบบออฟไลน์ ไม่มีเงื่อนไขเคร่งครัดด้านเวลาในการหาคำตอบ โดยฟังก์ชันวัตถุประสงค์เป็นแบบไม่เป็นเชิงเส้นและมักไม่เป็นแบบคอนเวกซ์ อาจมีตัวแปรตัดสินใจจำนวนมาก \n",
    "* ปัญหา RL โดยทั่วไปไม่จำเป็นต้องทำงานบนระบบที่ทรัพยากรจำกัด เช่นระบบฝังตัว   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/sharing-github/refs/heads/master/dewninja_logo50.jpg\" alt=\"dewninja\"/>\n",
    "</div>\n",
    "<div align=\"center\">dew.ninja 2025</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (controlenv)",
   "language": "python",
   "name": "controlennv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
