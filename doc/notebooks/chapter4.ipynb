{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "การควบคุมเหมาะที่สุดและการเรียนรูู้เสริมกำลัง -- ดร.วโรดม ตู้จินดา"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 การโปรแกรมพลวัตแบบเชิงกำหนด\n",
    "\n",
    "ในหัวข้อ 1.2.1 บทที่ 1 ได้กล่าวถึงการโปรแกรมพลวัต (dynamic programming ต่อไปจะเรียกย่อว่า DP) โดยสังเขป \n",
    "โดยเป็นวิธีการที่คิดค้นขึ้นตั้งแต่ยุคเริ่มต้นคอมพิวเตอร์เพื่อใช้เลือกการตัดสินใจ โดยมีการประยุกต์ใช้ในสาขาต่างๆ เช่นการวิจัยดำเนินการ (operation research มักเรียกโดยย่อว่า OR) ระบบควบคุม เศรษฐศาสตร์​ ฯลฯ แม้ว่าสมรรถนะของตัวประมวลผลสมัยนั้นยังไม่เพียงพอสำหรับการใช้งานในปัญหาทั่วไปที่มีข้อมูลจำนวนมากเหมือนในปัจจุบัน \n",
    "อย่างไรก็ตาม เบลแมน [1] ได้จัดรูปแบบปัญหาไว้เป็นอย่างดี \n",
    "ทำให้ถูกใช้เป็นเครื่องจักรสำคัญของ RL ตราบจนปัจจุบัน และมีการเชื่อมความสัมพันธ์กับปัญหาการควบคุมเหมาะที่สุด [2,3] \n",
    "โดยพบว่ามีหลักการที่สอดคล้องกันดังที่ได้อธิบายก่อนหน้านี้  \n",
    "\n",
    "แม้ว่าวิธีการที่อยู่บนพื้นฐาน DP จะแตกแขนงออกไปอย่างมากมายตามรูปแบบของโจทย์ปัญหา ในหนังสือนี้จะเน้นวิธีพื้นฐานที่ประยุกต์ใช้ในสาขาระบบควบคุมเป็นหลัก\n",
    "เนื้อหาในบทนี้ยังคงตามรอยรายวิชา [4] แต่เสริมด้วยเนื้อหาจาก [3,5] เพื่อให้เข้าใจความสัมพันธ์ระหว่าง DP, RL และการควบคุมเหมาะที่สุดได้ดีขึ้น\n",
    "\n",
    "## 4.1 คำศัพท์และสัญกรณ์\n",
    "\n",
    "ผู้เริ่มต้นที่ติดตามหนังสือและวีดีโอสอนเกี่ยวกับ DP อาจจะสับสนกับคำศัพท์และสัญกรณ์ที่ผู้นำเสนอใช้ที่มักจะแตกต่างกัน \n",
    "(ในตัวเลือกด้านล่างนี้ ตัวย่อเพิ่มเติมคือ OC = optimal control) \n",
    "ที่พบบ่อยเช่น\n",
    "\n",
    "#### ระบบเวลาต่อเนื่อง\n",
    "\n",
    "**หมายเหตุ :** โดยธรรมชาติแล้วปัญหา DP อยู่ในรูปแบบที่เป็นระบบดีสครีต การขยายไปยังระบบเวลาต่อเนื่องส่วนใหญ่จะเกี่ยวข้องกับ OC มากกว่า RL \n",
    "\n",
    "* สถานะ : $x(t)$\n",
    "* เอาต์พุตตัวควบคุม : $u(t)$\n",
    "* โมเดลพลวัต : $\\dot{x} = f(x(t),u(t))$ \n",
    "* ฟังก์ชันมูลค่า : $J(x), V(x)$\n",
    "* ฟังก์ชันมูลค่าเหมาะที่สุด : $J^*(x), V^*(x)$\n",
    "* ฟังก์ชันมูลค่าในแต่ละขั้น : $l(x,u), g(x,u)$\n",
    "\n",
    "#### ระบบเวลาดีสครีต\n",
    "\n",
    "* สถานะ (OC,RL) : $s[k], x[k], x_k$\n",
    "* เอาต์พุตตัวควบคุม (OC): $u[k], u_k$\n",
    "* ตัวกระทำ (RL): $a[k]$\n",
    "* นโยบายเหมาะที่สุด (RL): $\\pi[s_k]$\n",
    "* โมเดลพลวัต (OC,RL) : $f(x[k],u[k]), f(x_k,u_k), f(s[k],a[k])$ หรือโมเดลมาร์คอฟ (RL) : $p(s,s',a)$\n",
    "* ฟังก์ชันมูลค่า (OC,RL) : $J(x_k), J(s_k), V(x_k)$\n",
    "* ฟังก์ชันมูลค่าเหมาะที่สุด : $J^*(x_k), J^*(s_k), V^*(x_k)$\n",
    "* ฟังก์ชันมูลค่าในแต่ละขั้น (OC,RL): $l_k(x_k,u_k), g_k(x_k,u_k)$ \n",
    "\n",
    "**หมายเหตุ :** \n",
    "\n",
    "* ฟังก์ชันวัตถุประสงค์ใน (OC) ใช้ศัพท์ภาษาอังกฤษ \"cost function\" แต่ (RL) ใช้ \"value function\" \n",
    "ในหนังสือนี้เมื่อแปลเป็นภาษาไทยใช้คำว่า \"มูลค่า\" ทั้งหมด ซึ่งไม่สร้างปัญหาใดๆ หากแยกแยะได้ว่าวัตถุประสงค์ของ (OC) คือต้องการลดมูลค่า (เสมือนค่าใช้จ่าย) และ (RL) คือต้องการเพิ่มมูลค่า (เสมือนค่ารางวัล)  \n",
    "* เหตุผลการเปลี่ยนสัญกรในบางหนังสือหรือเอกสารคือไม่ต้องการให้ซ้ำกับอักขระที่ใช้อยู่ในบริบทอื่น เช่นหลีกเลี่ยงการใช้ชื่อ \"Q function\" เพราะไปซ้ำกับเมทริกซ์น้ำหนัก $Q$ ที่ใช้เดิมในฟังก์ชันมูลค่า [4] ขณะที่บางผู้สอนใช้อักขระ $x$ และ $s$ เพื่อแยกกันอย่างชัดเจนระหว่างระบบเวลาต่อเนื่องและดีสครีต [5]\n",
    "* การเลือกใช้โมเดลแบบใดขึ้นกับประเภทของปัญหา DP เช่นโมเดลมาร์คอฟ $p(s,s',a)$ ที่อยู่ในรูปการเปลี่ยนสถานะความน่าจะเป็น (transition probability) จะไม่เหมาะสมกับปัญหาเชิงกำหนด ขณะที่พลวัตในรูปสมการ $f(x_k,u_k)$ เหมาะสมกับการควบคุมเหมาะที่สุดเช่นหุ่นยนต์ แต่ไม่เข้ากันได้ดีกับปัญหา RL ในรูปแบบอื่นเช่นหมากรุก \n",
    "* นิยมใช้เครื่องหมาย $^*$ เป็นตัวยกกำลังแทนค่าที่เหมาะที่สุด เช่น $x_k^*, u_k^*, J^*(x_k,u_k)$ แต่ผู้สอนบางคนจะไม่ระบุในชั้นเรียนโดยอนุมานตามบริบทว่าเป็นค่าเหมาะที่สุด \n",
    "\n",
    "ศัพท์และสัญกรณ์ที่ใช้สำหรับเนื้อหาในหนังสือนี้\n",
    "\n",
    "(เบื้องต้น อาจมีการเปลี่ยนแปลงภายหลังตามความเหมาะสมเพื่อหลีกเลี่ยงการซ้ำกันของสัญกรณ์)\n",
    "\n",
    "#### ระบบเวลาต่อเนื่อง\n",
    "\n",
    "* สถานะ : $x(t)$\n",
    "* เอาต์พุต : $u(t)$\n",
    "* โมเดลพลวัต : $\\dot{x} = f(x(t),u(t))$ \n",
    "* ฟังก์ชันมูลค่า : $J(x)$\n",
    "* ฟังก์ชันมูลค่าเหมาะที่สุด : $J^*(x)$\n",
    "* ฟังก์ชันมูลค่าในแต่ละขั้น : $l(x,u)$\n",
    "\n",
    "#### ระบบเวลาดีสครีต\n",
    "\n",
    "* สถานะ : $x_k$\n",
    "* เอาต์พุตตัวควบคุม (OC): $u_k$\n",
    "* ตัวกระทำ (RL): $a_k$\n",
    "* นโยบายเหมาะที่สุด (RL): $\\pi[x_k]$\n",
    "* โมเดลพลวัต : $f(x_k,u_k)$\n",
    "* ฟังก์ชันมูลค่า : $J(x_k)$\n",
    "* ฟังก์ชันมูลค่าเหมาะที่สุด : $J^*(x_k)$\n",
    "* ฟังก์ชันมูลค่าในแต่ละขั้น (ตามบริบท): $l_k(x_k,u_k), g_k(x_k,u_k)$\n",
    "\n",
    "นอกจากนั้นจะเรียกบางคำศัพท์ให้สั้นลงเพีื่อความกระชับเข่น\n",
    "\n",
    "* เรียก $u(t), u_k$ ว่า ตัวควบคุม หมายความถึงเอาต์พุตจากตัวควบคุม \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 4.2 ประเภทของการโปรแกรมพลวัต\n",
    "\n",
    "หัวใจสำคัญของปัญหา DP คือระบบพลวัตในเวลาดีสครีตที่กำเนิดลำดับของสถานะภายใต้อิทธิพลของตัวควบคุมหรือตัวกระทำ \n",
    "สามารถแยกได้เป็น 2 ประเภทหลักคือ แบบเชิงกำหนด (deterministic) เมื่อการพัฒนาของตัวแปรภายในเป็นแบบกำหนดได้แน่นอน คือได้เอาต์พุตหรือคำตอบเหมือนเดิมจากอินพุตเดียวกันทุกครั้ง หรือแบบสโทแคสติก (stochastic)เมื่อมีการรบกวนแบบสุ่มทำให้คำตอบแตกต่างกันไปในแต่ละครั้ง \n",
    "\n",
    "นอกจากนั้นอาจแบ่งประเภทของปัญหา DP ในอีกมิติหนึ่ง คือแบบแม่นตรง (exact) หรือแบบใช้การประมาณค่า (approximate) ประเด็นหลักที่แตกต่างคือแบบแม่นตรงจะต้องการโมเดลทางคณิตศาสตร์ ส่วนแบบการประมาณค่าจะใช้โครงข่ายประสาทเทียมหรือสถาปัตยกรรมอื่นในการลดมิติของระบบ และใช้วิธีการจำลองโมเดลบนคอมพิวเตอร์แทนโมเดลทางคณิตศาสร์\n",
    "\n",
    "การศึกษาในบทนี้จะเริ่มจากปัญหา DP แบบเชิงกำหนดและแม่นตรงที่เป็นพื้นฐาน ก่อนจะขยายไปยังแบบอื่นในภายหลัง\n",
    "\n",
    "### 4.2.1 การโปรแกรมพลวัตแบบเชิงกำหนด\n",
    "\n",
    "ลำดับแรกในการศึกษาจะเริ่มต้นจากปัญหา DP แบบเชิงกำหนดและแม่นตรง คือใช้โมเดลทางคณิตศาสตร์ และไม่มีการรบกวนแบบสุ่มในพลวัต \n",
    "เพื่อความสะดวกในการอธิบาย จะขอกล่าวถึงหลักการของความเหมาะที่สุด (principle of optimality) ในบทที่ 1 อีกครั้งหนึ่ง เพราะเป็นหลักการสำคัญในการจัดรูป DP เพื่อหาคำตอบ\n",
    "\n",
    "<hr>\n",
    "นโยบายเหมาะที่สุดต้องมีคุณสมบัติคือ ไม่ว่าจะเลือกสถานะและการตัดสินใจเริ่มต้นอย่างไร \n",
    "การตัดสินใจครั้งต่อไปที่เหลือจะต้องยังคงเป็นนโยบายเหมาะที่สุดเสมอ เมื่อนับจากสถานะที่เกิดจากการตัดสินใจครั้งแรก\n",
    "<hr>\n",
    "\n",
    "จากรูปที่ 3.1 อธิบายได้ว่า สมมุติว่านโยบายในการเคลื่อนที่จากจุด $x_0$ ไปยังจุด $x_2$ โดยผ่านเส้นทาง (หรือแนววิถี) A \n",
    "คือนโยบายเหมาะที่สุดตามวัตถุประสงค์ที่กำหนด\n",
    "เมื่อพิจารณาปัญหาย่อยจากจุด $x_1$ ไปยังจุด $x_2$ เส้นทางที่เหมาะที่สุดก็ยังคงเป็นเส้นทางเดิมคือ A \n",
    "กล่าวคือจะไม่สามารถพบเส้นทางใหม่  B  ที่เหมาะที่สุดกว่าเส้นทาง A ได้ เพราะมิฉะนั้นในการหาค่าเหมาะที่สุดสำหรับปัญหารวม \n",
    "เราก็จะเลือกเส้นทางผ่าน B แทน​ A \n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/poo.png\" width=600 />\n",
    "\n",
    "รูปที่ 3.1 หลักการของความเหมาะที่สุด\n",
    "\n",
    "สังเกตจากรูปที่ 3.1 ว่าสถานะ $x_2$ เป็นจุดปลายของแนววิถี และปัญหาย่อย A คือจากจุดใดๆ ก่อนหน้าไปยังจุดปลาย เราไม่สามารถเลือก $x_2$ เป็นจุดอื่นในแนววิถีเพราะอาจมีการกำหนดมูลค่าระหว่างทางกับมูลค่าสุดท้ายแตกต่างกัน (ดังเช่นฟังก์ชันมูลค่าของปัญหา LQR ในบทที่ 3) ดังนั้นหลักการของความเหมาะที่สุดเสนอแนะให้เราแก้ปัญหาย่อยจากส่วนปลายย้อนกลับไปยังส่วนต้น จนกระทั่งครอบคลุมตลอดแนววิถีที่ต้องการ ซึ่งแนวทางนี้มีความสอดคล้องกับวิธีการยิงโดยอ้อมและการวนซ้ำริกคาติที่ได้ศึกษาในบทที่ 3 \n",
    "\n",
    "สิ่งสำคัญที่แตกต่างคือ คำตอบของวิธีก่อนหน้านี้จะเป็นแนววิถีชุดเดียวที่เริ่มต้นจากสถานะ $x_0$ แต่คำตอบของวิธี DP จะกว้างกว่า \n",
    "โดยครอบคลุมทุกแนววิถีที่เป็นไปได้ของระบบพลวัต แน่นอนสามารถหาคำตอบได้สำหรับปัญหาทั่วไปตราบเท่าที่เรามีคอมพิวเตอร์ที่มีสมรรถนะสูงเพียงพอจะคำนวณสำหรับจำนวนสถานะของปัญหานั้น ซึ่งในทางปฏิบัติเมื่อปัญหามีขนาดใหญ่ขึ้นจะเข้าสู่ขีดจำกัดที่เบลแมนเรียกว่า *คำสาปของมิติ (curse of dimensionality)* [1] เป็นเหตุผลที่ใช้ DP แบบประมาณค่าแทนแบบแม่นตรง แต่เพื่อความเข้าใจเบื้องต้นเราจะนำเสนอปัญหา DP แบบแม่นตรงก่อน โดยเริ่มจากปัญหาในรูปแบบดีสครีต"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 ปัญหาแบบแนวนอนจำกัด\n",
    "\n",
    "รูปแบบของปัญหาแบบแนวนอนจำกัด (finite horizon) คือเมื่อระบบวิวัฒน์ไปตามขั้นเวลาจำกัด N ขั้น (stages) ดังแสดงในรูปที่ 4.1 สถานะและตัวควบคุมของระบบ ณ ขั้นเวลา $k$ เขียนแทนด้วย $x_k$ และ $u_k$ ตามลำดับ โดยค่าของ $u_k$ ณ ขั้นเวลา $K$ ถูกเลือกจากซต $U_k(x_k)$ ในระบบเชิงกำหนด ค่าของ $x_{k+1}$ มิได้เป็นค่าสุ่ม แต่จะถูกกำหนดโดย $x_k$ และ $u_k$ สอดคล้องกับพลวัต\n",
    "\n",
    "$$\n",
    "x_{k+1} = f_k(x_k,u_k), \\;\\; k = 0,1,\\ldots, N-1 \\tag{4.1} \n",
    "$$\n",
    "\n",
    "นอกจากนั้นจะเห็นว่าแต่ละขั้นในรูปที่ 4.1 จะมีฟังก์ชันมูลค่า $g_k(x_k,u_k)$ ที่เรียกว่าเป็นมูลค่าการบวก (additive cost) เนื่องจากมูลค่าจะถูกสะสมตามขั้นเวลาไปจนสิ้นสุด ดังนั้นสำหรับค่าเริ่มต้น $x_0$ ที่กำหนด มูลค่ารวมของลำดับตัวควบคุม $\\{u_0, \\ldots, u_{N-1}\\}$ คือ\n",
    "\n",
    "$$\n",
    "J(x_0; u_0, \\ldots, u_{N-1}) = g_N(x_N) + \\sum_{k=0}^{N-1} g_k(x_k, u_k) \\tag{4.2}\n",
    "$$\n",
    "\n",
    "โดย $g_N(x_N)$ คือมูลค่าที่กำหนดให้กับส่วนปลายเมื่อกระบวนการสิ้นสุด \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/ch4_fhdp_stage.png\" width=800 />\n",
    "\n",
    "รูปที่ 4.1 รูปแบบปัญหาการควบคุมเหมาะที่สุดแบบแนวนอนจำกัด\n",
    "\n",
    "เราต้องการหาคำตอบต่ำสุดของ (4.2) สำหรับลำดับ $\\{u_0, \\ldots, u_{N-1}\\}$ ที่สอดคล้องกับเงื่อนไขบังคับ (4.1) \n",
    "\n",
    "$$\n",
    "J^*(x_0) = \\underset{\\substack{u_k \\in U_k(x_k) \\\\ k=0,\\ldots, N-1}}{min} J(x_0; u_0, \\ldots, u_{N-1}) \\tag{4.3}\n",
    "$$\n",
    "\n",
    "ข้อสังเกตหนึ่งสำหรับปัญหา DP ทั่วไปที่สอดคล้องกับการควบคุมเหมาะที่สุดคือ พลวัตของระบบจะวิวัฒน์ไปตามขั้นเวลา ดังนั้นตัวควบคุมในอนาคตไม่สามารถมีผลกระทบกับสถานะในอดีตได้ ตัวอย่างของแผนภาพการเปลี่ยนสถานะในปัญหา DP แบบแนวนอนจำกัดแสดงได้ดังรูปที่ 4.2 เริ่มจากสถานะ $x_0$ ที่กำหนด การเปลี่ยนสถานะจะขึ้นกับพลวัต $x_{k+1} = f_k(x_k,u_k)$ เมื่อเลือกตัวควบคุม $u_k \\in U_k$ \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/ch4_fsdp_diagram1.png\" width=800 />\n",
    "\n",
    "รูปที่ 4.2 แผนภาพการเปลี่ยนสถานะของปัญหา DP แบบแนวนอนจำกัด\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## บรรณานุกรม\n",
    "\n",
    "1. R.E. Bellman. Dynamic Programming. Princeton University Press. 1957.\n",
    "\n",
    "2. D.P. Bertsekas. Reinforcement Learning and Optimal Control. MIT Press. 2019.\n",
    "\n",
    "3. D.P. Bertsekas. [A Course in Reinforcement Learning, 2nd ed](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://web.mit.edu/dimitrib/www/RLCOURSECOMPLETE%202ndEDITION.pdf). Athena Scientific. 2025. \n",
    "\n",
    "4. Z. Manchester et.al. [16-745 Optimal Control & Reinforcement Learning, \n",
    "Course materials](https://optimalcontrol.ri.cmu.edu/#learning-resources), Carnegie Mellon University. 2024,2025.\n",
    "\n",
    "5. R. Tedrake. [Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and Manipulation (Course Notes for MIT 6.832)](https://underactuated.csail.mit.edu). 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/sharing-github/refs/heads/master/dewninja_logo50.jpg\" alt=\"dewninja\"/>\n",
    "</div>\n",
    "<div align=\"center\">dew.ninja 2025</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (controlenv)",
   "language": "python",
   "name": "controlennv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
