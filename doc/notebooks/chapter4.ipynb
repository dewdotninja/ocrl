{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "การควบคุมเหมาะที่สุดและการเรียนรูู้เสริมกำลัง -- ดร.วโรดม ตู้จินดา"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 การโปรแกรมพลวัตแบบเชิงกำหนด\n",
    "\n",
    "ในหัวข้อ 1.2.1 บทที่ 1 ได้กล่าวถึงการโปรแกรมพลวัต (dynamic programming ต่อไปจะเรียกย่อว่า DP) โดยสังเขป \n",
    "โดยเป็นวิธีการที่คิดค้นขึ้นตั้งแต่ยุคเริ่มต้นคอมพิวเตอร์เพื่อใช้เลือกการตัดสินใจ โดยมีการประยุกต์ใช้ในสาขาต่างๆ เช่นการวิจัยดำเนินการ (operation research มักเรียกโดยย่อว่า OR) ระบบควบคุม เศรษฐศาสตร์​ ฯลฯ แม้ว่าสมรรถนะของตัวประมวลผลสมัยนั้นยังไม่เพียงพอสำหรับการใช้งานในปัญหาทั่วไปที่มีข้อมูลจำนวนมากเหมือนในปัจจุบัน \n",
    "อย่างไรก็ตาม เบลแมน [1] ได้จัดรูปแบบปัญหาไว้เป็นอย่างดี \n",
    "ทำให้ถูกใช้เป็นเครื่องจักรสำคัญของ RL ตราบจนปัจจุบัน และมีการเชื่อมความสัมพันธ์กับปัญหาการควบคุมเหมาะที่สุด [2,3] \n",
    "โดยพบว่ามีหลักการที่สอดคล้องกันดังที่ได้อธิบายก่อนหน้านี้  \n",
    "\n",
    "แม้ว่าวิธีการที่อยู่บนพื้นฐาน DP จะแตกแขนงออกไปอย่างมากมายตามรูปแบบของโจทย์ปัญหา ในหนังสือนี้จะเน้นวิธีพื้นฐานที่ประยุกต์ใช้ในสาขาระบบควบคุมเป็นหลัก\n",
    "เนื้อหาในบทนี้ยังคงตามรอยรายวิชา [4] แต่เสริมด้วยเนื้อหาจาก [3,5] เพื่อให้เข้าใจความสัมพันธ์ระหว่าง DP, RL และการควบคุมเหมาะที่สุดได้ดีขึ้น\n",
    "\n",
    "## 4.1 คำศัพท์และสัญกรณ์\n",
    "\n",
    "ผู้เริ่มต้นที่ติดตามหนังสือและวีดีโอสอนเกี่ยวกับ DP อาจจะสับสนกับคำศัพท์และสัญกรณ์ที่ผู้นำเสนอใช้ที่มักจะแตกต่างกัน \n",
    "(ในตัวเลือกด้านล่างนี้ ตัวย่อเพิ่มเติมคือ OC = optimal control) \n",
    "ที่พบบ่อยเช่น\n",
    "\n",
    "#### ระบบเวลาต่อเนื่อง\n",
    "\n",
    "**หมายเหตุ :** โดยธรรมชาติแล้วปัญหา DP อยู่ในรูปแบบที่เป็นระบบดีสครีต การขยายไปยังระบบเวลาต่อเนื่องส่วนใหญ่จะเกี่ยวข้องกับ OC มากกว่า RL \n",
    "\n",
    "* สถานะ : $x(t)$\n",
    "* เอาต์พุตตัวควบคุม : $u(t)$\n",
    "* โมเดลพลวัต : $\\dot{x} = f(x(t),u(t))$ \n",
    "* ฟังก์ชันมูลค่า : $J(x), V(x)$\n",
    "* ฟังก์ชันมูลค่าเหมาะที่สุด : $J^*(x), V^*(x)$\n",
    "* ฟังก์ชันมูลค่าในแต่ละขั้น : $l(x,u), g(x,u)$\n",
    "\n",
    "#### ระบบเวลาดีสครีต\n",
    "\n",
    "* สถานะ (OC,RL) : $s[k], x[k], x_k$\n",
    "* เอาต์พุตตัวควบคุม (OC): $u[k], u_k$\n",
    "* ตัวกระทำ (RL): $a[k]$\n",
    "* นโยบายเหมาะที่สุด (RL): $\\pi[s_k]$\n",
    "* โมเดลพลวัต (OC,RL) : $f(x[k],u[k]), f(x_k,u_k), f(s[k],a[k])$ หรือโมเดลมาร์คอฟ (RL) : $p(s,s',a)$\n",
    "* ฟังก์ชันมูลค่า (OC,RL) : $J(x_k), J(s_k), V(x_k)$\n",
    "* ฟังก์ชันมูลค่าเหมาะที่สุด : $J^*(x_k), J^*(s_k), V^*(x_k)$\n",
    "* ฟังก์ชันมูลค่าในแต่ละขั้น (OC,RL): $l_k(x_k,u_k), g_k(x_k,u_k)$ \n",
    "\n",
    "**หมายเหตุ :** \n",
    "\n",
    "* ฟังก์ชันวัตถุประสงค์ใน (OC) ใช้ศัพท์ภาษาอังกฤษ \"cost function\" แต่ (RL) ใช้ \"value function\" \n",
    "ในหนังสือนี้เมื่อแปลเป็นภาษาไทยใช้คำว่า \"มูลค่า\" ทั้งหมด ซึ่งไม่สร้างปัญหาใดๆ หากแยกแยะได้ว่าวัตถุประสงค์ของ (OC) คือต้องการลดมูลค่า (เสมือนค่าใช้จ่าย) และ (RL) คือต้องการเพิ่มมูลค่า (เสมือนค่ารางวัล)  \n",
    "* เหตุผลการเปลี่ยนสัญกรในบางหนังสือหรือเอกสารคือไม่ต้องการให้ซ้ำกับอักขระที่ใช้อยู่ในบริบทอื่น เช่นหลีกเลี่ยงการใช้ชื่อ \"Q function\" เพราะไปซ้ำกับเมทริกซ์น้ำหนัก $Q$ ที่ใช้เดิมในฟังก์ชันมูลค่า [4] ขณะที่บางผู้สอนใช้อักขระ $x$ และ $s$ เพื่อแยกกันอย่างชัดเจนระหว่างระบบเวลาต่อเนื่องและดีสครีต [5]\n",
    "* การเลือกใช้โมเดลแบบใดขึ้นกับประเภทของปัญหา DP เช่นโมเดลมาร์คอฟ $p(s,s',a)$ ที่อยู่ในรูปการเปลี่ยนสถานะความน่าจะเป็น (transition probability) จะไม่เหมาะสมกับปัญหาเชิงกำหนด ขณะที่พลวัตในรูปสมการ $f(x_k,u_k)$ เหมาะสมกับการควบคุมเหมาะที่สุดเช่นหุ่นยนต์ แต่ไม่เข้ากันได้ดีกับปัญหา RL ในรูปแบบอื่นเช่นหมากรุก \n",
    "* นิยมใช้เครื่องหมาย $^*$ เป็นตัวยกกำลังแทนค่าที่เหมาะที่สุด เช่น $x_k^*, u_k^*, J^*(x_k,u_k)$ แต่ผู้สอนบางคนจะไม่ระบุในชั้นเรียนโดยอนุมานตามบริบทว่าเป็นค่าเหมาะที่สุด \n",
    "\n",
    "ศัพท์และสัญกรณ์ที่ใช้สำหรับเนื้อหาในหนังสือนี้\n",
    "\n",
    "(เบื้องต้น อาจมีการเปลี่ยนแปลงภายหลังตามความเหมาะสมเพื่อหลีกเลี่ยงการซ้ำกันของสัญกรณ์)\n",
    "\n",
    "#### ระบบเวลาต่อเนื่อง\n",
    "\n",
    "* สถานะ : $x(t)$\n",
    "* เอาต์พุต : $u(t)$\n",
    "* โมเดลพลวัต : $\\dot{x} = f(x(t),u(t))$ \n",
    "* ฟังก์ชันมูลค่า : $J(x)$\n",
    "* ฟังก์ชันมูลค่าเหมาะที่สุด : $J^*(x)$\n",
    "* ฟังก์ชันมูลค่าในแต่ละขั้น : $l(x,u)$\n",
    "\n",
    "#### ระบบเวลาดีสครีต\n",
    "\n",
    "* สถานะ : $x_k$\n",
    "* เอาต์พุตตัวควบคุม (OC): $u_k$\n",
    "* ตัวกระทำ (RL): $a_k$\n",
    "* นโยบายเหมาะที่สุด (RL): $\\pi[x_k]$\n",
    "* โมเดลพลวัต : $f(x_k,u_k)$\n",
    "* ฟังก์ชันมูลค่า : $J(x_k)$\n",
    "* ฟังก์ชันมูลค่าเหมาะที่สุด : $J^*(x_k)$\n",
    "* ฟังก์ชันมูลค่าในแต่ละขั้น (ตามบริบท): $l_k(x_k,u_k), g_k(x_k,u_k)$\n",
    "\n",
    "นอกจากนั้นจะเรียกบางคำศัพท์ให้สั้นลงเพีื่อความกระชับเข่น\n",
    "\n",
    "* เรียก $u(t), u_k$ ว่า ตัวควบคุม หมายความถึงเอาต์พุตจากตัวควบคุม \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 4.2 ประเภทของการโปรแกรมพลวัต\n",
    "\n",
    "หัวใจสำคัญของปัญหา DP คือระบบพลวัตในเวลาดีสครีตที่กำเนิดลำดับของสถานะภายใต้อิทธิพลของตัวควบคุมหรือตัวกระทำ \n",
    "สามารถแยกได้เป็น 2 ประเภทหลักคือ แบบเชิงกำหนด (deterministic) เมื่อการพัฒนาของตัวแปรภายในเป็นแบบกำหนดได้แน่นอน คือได้เอาต์พุตหรือคำตอบเหมือนเดิมจากอินพุตเดียวกันทุกครั้ง หรือแบบสโทแคสติก (stochastic)เมื่อมีการรบกวนแบบสุ่มทำให้คำตอบแตกต่างกันไปในแต่ละครั้ง \n",
    "\n",
    "นอกจากนั้นอาจแบ่งประเภทของปัญหา DP ในอีกมิติหนึ่ง คือแบบแม่นตรง (exact) หรือแบบใช้การประมาณค่า (approximate) ประเด็นหลักที่แตกต่างคือแบบแม่นตรงจะต้องการโมเดลทางคณิตศาสตร์ ส่วนแบบการประมาณค่าจะใช้โครงข่ายประสาทเทียมหรือสถาปัตยกรรมอื่นในการลดมิติของระบบ และใช้วิธีการจำลองโมเดลบนคอมพิวเตอร์แทนโมเดลทางคณิตศาสร์\n",
    "\n",
    "การศึกษาในบทนี้จะเริ่มจากปัญหา DP แบบเชิงกำหนดและแม่นตรงที่เป็นพื้นฐาน ก่อนจะขยายไปยังแบบอื่นในภายหลัง\n",
    "\n",
    "### 4.2.1 การโปรแกรมพลวัตแบบเชิงกำหนด\n",
    "\n",
    "ลำดับแรกในการศึกษาจะเริ่มต้นจากปัญหา DP แบบเชิงกำหนดและแม่นตรง คือใช้โมเดลทางคณิตศาสตร์ และไม่มีการรบกวนแบบสุ่มในพลวัต \n",
    "เพื่อความสะดวกในการอธิบาย จะขอกล่าวถึงหลักการของความเหมาะที่สุด (principle of optimality) ในบทที่ 1 อีกครั้งหนึ่ง เพราะเป็นหลักการสำคัญในการจัดรูป DP เพื่อหาคำตอบ\n",
    "\n",
    "<hr>\n",
    "นโยบายเหมาะที่สุดต้องมีคุณสมบัติคือ ไม่ว่าจะเลือกสถานะและการตัดสินใจเริ่มต้นอย่างไร \n",
    "การตัดสินใจครั้งต่อไปที่เหลือจะต้องยังคงเป็นนโยบายเหมาะที่สุดเสมอ เมื่อนับจากสถานะที่เกิดจากการตัดสินใจครั้งแรก\n",
    "<hr>\n",
    "\n",
    "จากรูปที่ 3.1 อธิบายได้ว่า สมมุติว่านโยบายในการเคลื่อนที่จากจุด $x_0$ ไปยังจุด $x_2$ โดยผ่านเส้นทาง (หรือแนววิถี) A \n",
    "คือนโยบายเหมาะที่สุดตามวัตถุประสงค์ที่กำหนด\n",
    "เมื่อพิจารณาปัญหาย่อยจากจุด $x_1$ ไปยังจุด $x_2$ เส้นทางที่เหมาะที่สุดก็ยังคงเป็นเส้นทางเดิมคือ A \n",
    "กล่าวคือจะไม่สามารถพบเส้นทางใหม่  B  ที่เหมาะที่สุดกว่าเส้นทาง A ได้ เพราะมิฉะนั้นในการหาค่าเหมาะที่สุดสำหรับปัญหารวม \n",
    "เราก็จะเลือกเส้นทางผ่าน B แทน​ A \n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/poo.png\" width=600 />\n",
    "\n",
    "รูปที่ 3.1 หลักการของความเหมาะที่สุด\n",
    "\n",
    "สังเกตจากรูปที่ 3.1 ว่าสถานะ $x_2$ เป็นจุดปลายของแนววิถี และปัญหาย่อย A คือจากจุดใดๆ ก่อนหน้าไปยังจุดปลาย เราไม่สามารถเลือก $x_2$ เป็นจุดอื่นในแนววิถีเพราะอาจมีการกำหนดมูลค่าระหว่างทางกับมูลค่าสุดท้ายแตกต่างกัน (ดังเช่นฟังก์ชันมูลค่าของปัญหา LQR ในบทที่ 3) ดังนั้นหลักการของความเหมาะที่สุดเสนอแนะให้เราแก้ปัญหาย่อยจากส่วนปลายย้อนกลับไปยังส่วนต้น จนกระทั่งครอบคลุมตลอดแนววิถีที่ต้องการ ซึ่งแนวทางนี้มีความสอดคล้องกับวิธีการยิงโดยอ้อมและการวนซ้ำริกคาติที่ได้ศึกษาในบทที่ 3 \n",
    "\n",
    "สิ่งสำคัญที่แตกต่างคือ คำตอบของวิธีก่อนหน้านี้จะเป็นแนววิถีชุดเดียวที่เริ่มต้นจากสถานะ $x_0$ แต่คำตอบของวิธี DP จะกว้างกว่า \n",
    "โดยครอบคลุมทุกแนววิถีที่เป็นไปได้ของระบบพลวัต แน่นอนสามารถหาคำตอบได้สำหรับปัญหาทั่วไปตราบเท่าที่เรามีคอมพิวเตอร์ที่มีสมรรถนะสูงเพียงพอจะคำนวณสำหรับจำนวนสถานะของปัญหานั้น ซึ่งในทางปฏิบัติเมื่อปัญหามีขนาดใหญ่ขึ้นจะเข้าสู่ขีดจำกัดที่เบลแมนเรียกว่า *คำสาปของมิติ (curse of dimensionality)* [1] เป็นเหตุผลที่ใช้ DP แบบประมาณค่าแทนแบบแม่นตรง แต่เพื่อความเข้าใจเบื้องต้นเราจะนำเสนอปัญหา DP แบบแม่นตรงก่อน โดยเริ่มจากปัญหาในรูปแบบดีสครีต"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 ปัญหาแบบแนวนอนจำกัด\n",
    "\n",
    "รูปแบบของปัญหาแบบแนวนอนจำกัด (finite horizon) คือเมื่อระบบวิวัฒน์ไปตามขั้นเวลาจำกัด N ขั้น (stages) ดังแสดงในรูปที่ 4.1 สถานะและตัวควบคุมของระบบ ณ ขั้นเวลา $k$ เขียนแทนด้วย $x_k$ และ $u_k$ ตามลำดับ โดยค่าของ $u_k$ ณ ขั้นเวลา $K$ ถูกเลือกจากซต $U_k(x_k)$ ในระบบเชิงกำหนด ค่าของ $x_{k+1}$ มิได้เป็นค่าสุ่ม แต่จะถูกกำหนดโดย $x_k$ และ $u_k$ สอดคล้องกับพลวัต\n",
    "\n",
    "$$\n",
    "x_{k+1} = f_k(x_k,u_k), \\;\\; k = 0,1,\\ldots, N-1 \\tag{4.1} \n",
    "$$\n",
    "\n",
    "นอกจากนั้นจะเห็นว่าแต่ละขั้นในรูปที่ 4.1 จะมีฟังก์ชันมูลค่า $g_k(x_k,u_k)$ ที่เรียกว่าเป็นมูลค่าการบวก (additive cost) เนื่องจากมูลค่าจะถูกสะสมตามขั้นเวลาไปจนสิ้นสุด ดังนั้นสำหรับค่าเริ่มต้น $x_0$ ที่กำหนด มูลค่ารวมของลำดับตัวควบคุม $\\{u_0, \\ldots, u_{N-1}\\}$ คือ\n",
    "\n",
    "$$\n",
    "J(x_0; u_0, \\ldots, u_{N-1}) = g_N(x_N) + \\sum_{k=0}^{N-1} g_k(x_k, u_k) \\tag{4.2}\n",
    "$$\n",
    "\n",
    "โดย $g_N(x_N)$ คือมูลค่าที่กำหนดให้กับส่วนปลายเมื่อกระบวนการสิ้นสุด \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/ch4_fhdp_stage.png\" width=1000 />\n",
    "\n",
    "รูปที่ 4.1 รูปแบบปัญหาการควบคุมเหมาะที่สุดแบบแนวนอนจำกัด\n",
    "\n",
    "เราต้องการหาคำตอบต่ำสุดของ (4.2) สำหรับลำดับ $\\{u_0, \\ldots, u_{N-1}\\}$ ที่สอดคล้องกับเงื่อนไขบังคับ (4.1) \n",
    "\n",
    "$$\n",
    "J^*(x_0) = \\underset{\\substack{u_k \\in U_k(x_k) \\\\ k=0,\\ldots, N-1}}{min} J(x_0; u_0, \\ldots, u_{N-1}) \\tag{4.3}\n",
    "$$\n",
    "\n",
    "ข้อสังเกตหนึ่งสำหรับปัญหา DP ทั่วไปที่สอดคล้องกับการควบคุมเหมาะที่สุดคือ พลวัตของระบบจะวิวัฒน์ไปตามขั้นเวลา ดังนั้นตัวควบคุมในอนาคตไม่สามารถมีผลกระทบกับสถานะในอดีตได้ ตัวอย่างของแผนภาพการเปลี่ยนสถานะในปัญหา DP แบบแนวนอนจำกัดแสดงได้ดังรูปที่ 4.2 เริ่มจากสถานะ $x_0$ ที่กำหนด การเปลี่ยนสถานะจะขึ้นกับพลวัต $x_{k+1} = f_k(x_k,u_k)$ เมื่อเลือกตัวควบคุม $u_k \\in U_k$ ขณะที่มูลค่าในการเปลี่ยนสถานะของแต่ละขั้นถูกคำนวณโดยฟังก์ชัน $g_k(x_k,u_k)$ สำหรับขั้นสุดท้าย เราเพิ่มโนดสื้นสุดเทียม T เข้าไปในแผนภาพ โดยเส้นทางจากสถานะ $x_N$ ไปยังโนด T มีมูลค่าเท่ากับ $g_N(x_N)$\n",
    "\n",
    "สังเกตว่าสำหรับสถานะ $x_0$ ที่กำหนด ลำดับของตัวควบคุม $\\{u_0, \\ldots, u_{N-1}\\}$ ที่เลือกทำให้สถานะเปลี่ยนตามแนววิถีหนึ่งจากแนววิถีทั้งหมดที่เป็นไปได้ และสิ้นสุดที่สถานะ $x_N$ โนดหนึ่งในขั้น $N$ เพื่อความเข้าใจที่ง่ายขึ้น สมมุติว่าเรามองมูลค่าของเส้นเปลี่ยนสถานะในแต่ละขั้นเป็นความยาวของเส้นนั้น จะเห็นได้ว่าการแป้ปัญหา DP เชิงกำหนดแบบแนวนอนจำกัดก็คือการหาเส้นทางรวมที่สั้นที่สุดจากโนดเริ่มต้นสู่โนดปลายนั่นเอง โดยความยาวของเส้นทางรวมคือผลรวมของความยาวของเส้นระหว่างขั้น \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/ch4_fsdp_diagram1.png\" width=1000 />\n",
    "\n",
    "รูปที่ 4.2 แผนภาพการเปลี่ยนสถานะของปัญหา DP แบบแนวนอนจำกัด\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 4.3 อัลกอริทึมการโปรแกรมพลวัต \n",
    "\n",
    "เมิื่อเข้าใจรูปแบบปัญหา DP เราสามารถสร้างอัลกอริทึมสำหรับแก้ปัญหาเพื่อหาคำตอบ กล่าวโดยทั่วไปคือต้องการแก้ปัญหาการตัดสินใจโดยลำดับสำหรับ \n",
    "N ขั้น โดยแยกปัญหาเป็นลำดับของปัญหาย่อยในแต่ละขั้น เป้าหมายของอัลกอริทึมคือลำดับของตัวควบคุมเหมาะที่สุด $u_0^*, \\ldots , u_{N-1}^*$ ที่ทำให้ลำดับของฟังก์ชันมูลค่า​ $J_0^*, \\ldots, J_{N-1}^*$ มีค่าเหมาะ(ต่ำ)ที่สุด เริ่มต้นจากฟังก์ชันมูลค่าส่วนปลายคือ $J_N^* = g_N(x_N)$ คำนวณ $J_{N-1}^*$ โดยแก้ปัญหาการตัดสินใจขั้นเดี่ยวที่ตัวแปรเหมาะที่สุดคือ $u_{N-1}$ ต่อมาใช้ $J_{N-1}^*$ \n",
    "เพื่อคำนวณ $J_{N-2}^*$ ดำเนินการเช่นนี้อย่างต่อเนื่องในการคำนวณ $J_{N-3}^*,\\ldots, J_0^*$\n",
    "\n",
    "สังเกตว่าการคำนวณนี้มีรูปแบบย้อนหลังจากส่วนปลายมายังส่วนต้น ซึ่งอาศัยหลักการของความเหมาะที่สุดนั่นเอง โดยในระบบดีสครึตจะเขียนให้ชัดเจนขึ้น ดูรูปที่ 4.3 ประกอบ\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/ch4_poo_discrete.png\" width=800 />\n",
    "\n",
    "รูปที่ 4.3 แผนภาพแสดงหลักการของเความเหมาะที่สุดในระบบดีสครีต \n",
    "\n",
    "<hr>\n",
    "\n",
    "**หลักการของความเหมาะที่สุด**\n",
    "\n",
    "สำหรับลำดับตัวควบคุมเหมาะที่สุด $\\{u_0^*, \\ldots, u_{N-1}^*\\}$ และค่าเริ่มต้น $x_0$ เป็นตัวกำหนดลำดับของสถานะ \n",
    "$\\{x_1^*, \\ldots, x_{N}^*\\}$ ตามสมการพลวัต (4.1) พิจารณาปัญหาย่อยที่เริ่มจาก $x_k^*$ ณ ขั้นเวลา $k$ ต้องการหาค่าต่ำสุดของ \n",
    "*มูลค่าการไป (cost-to-go)* จากขั้น $k$ ไปยังขั้นปลาย $N$\n",
    "\n",
    "$$\n",
    "g_k(x_k^*,u_k) + \\sum_{m = k+1}^{N-1}g_m(x_m, u_m) + g_N(x_N) \\tag{4.4}\n",
    "$$\n",
    "\n",
    "โดยลำดับตัวควบคุม $\\{u_k,\\ldots,u_{N-1}\\}$ ในเซต $u_m \\in U_m(x_m), \\; m = k, \\ldots, N-1$ ดังนั้นลำดับตัวควบคุมเหมาะที่สุดในส่วนปลายที่ถูกตัดออก $\\{u_k^*,\\ldots,u_{N-1}^*\\}$ จะเหมาะที่สุดสำหรับปัญหาย่อยนี้ด้วย\n",
    "\n",
    "<hr>\n",
    "\n",
    "เราเรียกปัญหาย่อยนี้ว่า ปัญหาย่อยส่วนปลาย ที่เริ่มต้นจาก $x_k^*$ หลักการของความเหมาะที่สุดกล่าวว่า ส่วนปลายของลำดับที่เหมาะที่สุดจะเหมาะที่สุดสำหรับปัญหาย่อยส่วนปลาย โดยเหตุผลเดิมที่อธิบายแล้วข้างต้น หากลำดับตัวควบคุมส่วนปลายที่ถูกตัดออก $\\{u_k^*,\\ldots,u_{N-1}^*\\}$ ไม่เป็นแบบเหมาะที่สุด เราต้องสามารถจะลดมูลค่าลงได้อีกโดยเลือกลำดับตัวควบคุมอื่นเมื่อถึงขั้น $x_k^*$ ทั้งนี้เนื่องจากตัวควบคุมที่เลือกก่อนหน้านี้คือ $\\{u_0^*,\\ldots,u_{k-1}^*\\}$ มิได้จำกัดการเลือกตัวควบคุมในอนาคต\n",
    "\n",
    "โดยหลักการของความเหมาะที่สุด อัลกอริทึม DP จะเริ่มต้นจากการคำนวณค่าเหมาะที่สุดของฟังก์ชันมูลค่า \n",
    "$$\n",
    "J_N^*(x_N), J_{N-1}^*(x_{N-1}), \\ldots, J_0^*(x_0) \n",
    "$$\n",
    "ตามลำดับ เริ่มต้นจาก $J_N^*$ ย้อนหลังไปยัง $J_{N-1}^*, J_{N-2}^*$ จนถึง $J_0^*$ ดังนั้นเขียนเป็นโครงสร้างได้ดังนี้\n",
    "\n",
    "<hr>\n",
    "\n",
    "**อัลกอริทึม DP สำหรับปัญหาเชิงกำหนดแบบแนวนอนจำกัด**\n",
    "\n",
    "เริ่มต้นจาก\n",
    "$$\n",
    "J_N^*(x_N) = g_N(x_N),\\; \\forall x_N \\tag{4.5}\n",
    "$$\n",
    "\n",
    "วนซ้ำโดยลดค่า $k$ จาก $N-1$ ถึง $0$\n",
    "\n",
    "$$\n",
    "J^*(x_k) = \\underset{u_k \\in U_k(x_k)}{min} \\left[g_k(x_k,u_k) + J_{k+1}^*(f_k(x_k,u_k))\\right],\\; \\forall x_k \\tag{4.6}\n",
    "$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "รูปที่ 4.4 อธิบายการหาค่าของ $J_k^*(x_k)$ ตาม (4.6) ซึ่งก็คือมูลค่าเหมาะที่สุดของปัญหาย่อยส่วนปลายที่เริ่มจากสถานะ $x_k$ \n",
    "ณ ขั้นเวลา $k$ มีมูลค่าการไปตาม (4.4) ในการหาคำตอบเราเพียงเลือกตัวควบคุม $u_k$ \n",
    "ที่จะให้ค่าต่ำสุดสำหรับมูลค่าขั้นเดี่ยวแรกบวกกับมูลค่าต่ำสุดของปัญหาย่อยส่วนปลาย สังเกตว่า ณ ขั้น $k$ การคำนวณ (4.6) \n",
    "ต้องกระทำสำหรับทุกสถานะ $x_k$ ก่อนจะย้อนหลังมายังขั้น $k-1$ \n",
    "\n",
    "สำหรับการพิสูจน์ว่า $J_k^*(x_k)$ เป็นมูลค่าเหมาะที่สุดของปัญหาส่วนปลาย $N-k$ ขั้นจาก $x_k$ ไปยัง $x_N$ โดยวิธีอุปนัย \n",
    "(induction) สามารถศึกษาได้จาก [3]\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/ch4_tail_subprob.png\" width=800 />\n",
    "\n",
    "รูปที่ 4.4 มูลค่าเหมาะที่สุด $J_k^*(x_k)$ ของปัญหาย่อยส่วนปลาย\n",
    "\n",
    "**หมายเหตุุ** $J_k^*(x_k)$ หรืออาจเขียนย่อเพียง $J_k^*$ มักถูกเรียกว่า \"optimal cost-to-go\" \n",
    "ซึ่งผู้เขียนใช้ภาษาไทยว่า \"มูลค่าการไปเหมาะที่สุด\" หรือ \"มูลค่าเหมาะที่สุดที่จะไป\" ที่ยังไม่ค่อยถูกใจนัก \n",
    "อยากได้คำที่สื่อความหมายดีกว่านี้ คำศัพท์อื่นที่ใช้เรียกเช่น ฟังก์ชันมูลค่าเหมาะที่สุด (optimal cost function) \n",
    "หรือในปัญหา RL ที่เป็นการหาค่าสูงสุด มักใช้ภาษาอังกฤษว่า \"optimal value function\" ณ เวลา $k$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "จากแผนภาพในรูปที่ 4.2 สมมุติว่าเมื่อใช้อัลกอริทึม DP คำนวณมูลค่าย้อนหลังจากปลายมาสู่สถานะเริ่มต้นได้ดังรูปที่ 4.5 \n",
    "ซึ่งเป็นเสมือนขั้นแรกในการแก้ปํญหาเท่านั้น เพราะในรูปปัญหาขนาดเล็กที่ใช้อธิบายนี้เหมือนกับว่าเราได้เห็นแนววิถีตัวควบคุมเหมาะที่สุดตามเส้นสีแดง \n",
    "แต่ในปัญหาจริงที่มีขนาดใหญ่เราอาจไม่ได้มีการเก็บค่าของตัวควบคุมทั้งหมดไว้ \n",
    " \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/ch4_fsdp_diagram2.png\" width=800 />\n",
    "\n",
    "รููปที่ 4.5 การคำนวณมูลค่าการไปเหมาะที่สุดจากส่วนปลายย้อนมายังจุดเริ่มต้น\n",
    "\n",
    "ดังนั้นขั้นตอนที่สองคือคำนวณหาแนววิถีของตัวควบคุมเหมาะที่สุดจากสถานะเริ่มต้นไปข้างหน้าสู่สถานะปลายดังแสดงในรูปที่ 4.6\n",
    "\n",
    "<hr>\n",
    "\n",
    "**การหาลำดับตัวควบคุมเหมาะที่สุด**\n",
    "\n",
    "ในการหาลำดับของตัวควบคุมเหมาะที่สุด $\\{u_0^*, \\ldots, u_{N-1}^*\\}$ กำหนด \n",
    "\n",
    "$$\n",
    "u_0^* = arg \\; \\underset{u_0 \\in U_0(x_0)}{min} \\left[g_0(x_0,u_0) + J_1^*(f_0(x_0,u_0))\\right] \\tag{4.7}\n",
    "$$\n",
    "และ\n",
    "$$\n",
    "x_1^* = f_0(x_0,u_0^*) \\tag{4.8}\n",
    "$$\n",
    "\n",
    "วนซ้ำในทิศทางข้างหน้า จาก $k = 1,2,\\ldots, N-1$ \n",
    "\n",
    "$$\n",
    "u_k^* = arg \\; \\underset{u_k \\in U_k(x_k^*)}{min} \\left[g_k(x_k^*,u_k) + J_{k+1}^*(f_k(x_k^*,u_k))\\right] \\tag{4.9}\n",
    "$$\n",
    "และ\n",
    "$$\n",
    "x_{k+1}^* = f_k(x_k^*,u_k^*) \\tag{4.10}\n",
    "$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/ch4_fsdp_diagram3.png\" width=800 />\n",
    "\n",
    "รููปที่ 4.6 การคำนวณตัวควบคุมเหมาะที่สุด $\\{u_0^*, \\ldots, u_{N-1}^*\\}$ ในทิศทางข้างหน้า\n",
    "\n",
    "**ตัวอย่าง 4.1** \n",
    "\n",
    "**หมายเหตุ :** ตัวอย่างนี้ตามรอยหนังสือ [3] เพียงเปลี่ยนข้อมูลค่าใช้จ่ายระหว่างเมือง\n",
    "\n",
    "ปัญหาการเดินทางของพนักงานขาย (traveling salesman problem ต่อไปจะเรียกย่อว่า TSP) เป็นที่รู้จักกันดีโดยเฉพาะผู้ศึกษาด้านคอมพิวเตอร์\n",
    " โจทย์กำหนดรายชื่อเมืองจำนวนหนึ่ง และระยะทาง (หรือบางโจทย์กำหนดเป็นค่าใช้จ่ายในการเดินทาง) ระหว่างแต่ละคู่ของเมืองทั้งหมด \n",
    " ต้องการหาเส้นทางที่สั้นที่สุด (หรือค่าใช้จ่ายต่ำสุด) หากต้องการแวะที่แต่ละเมืองเพียงครั้งเดียว และกลับมาที่เมืองเริ่มต้น\n",
    "\n",
    "ในตัวอย่างนี้จะลองศึกษาการแก้ปัญหา TSP ขนาดเล็กโดยอัลกอริทึม DP โดยกำหนด 4 เมืองคือ A,B,C,D \n",
    "ในการแปลงโจทย์นี้ให้กลายเป็นปัญหา DP \n",
    "เราจะสร้างแผนภาพที่แต่ละโนดแทนลำดับที่แตกต่างกันของเมืองที่แวะไป โดยเราเลือกจะเริ่มจากเมือง A \n",
    "แล้วสิ้นสุดที่การกลับมาที่เมือง A ดังนั้นแผนภาพรวมจะเป็นดังรูปที่ 4.7 \n",
    "สำหรับเมทริกซ์ทางด้านขวาของรูปแสดงค่าใช้จ่ายในการเดินทางระหว่างเมือง ซึ่งในกรณีนี้เป็นแบบสมมาตร \n",
    "กล่าวคือค่าใช้จ่ายในการเดินทางระหว่างสองเมืองใดๆ ไม่ขึ้นกับเริ่มต้นและสิ้นสุดที่เมืองใด \n",
    "ซึ่งในกรณีทั่วไปหากไม่เป็นแบบสมมาตรก็ยังสามารถใช้วิธี DP ในการหาคำตอบได้\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/ch4_travel_salesman.png\" width=600 />\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/ocrl/refs/heads/main/doc/figs/ch4_intercity_travel.png\" width=250 />\n",
    "\n",
    "รููปที่ 4.7 การจัดรูป TSP เพื่อแก้ปัญหาโดยวิธี DP\n",
    "\n",
    "ในแผนภาพรูปที่ 4.7 ยังได้แสดงถึงารคำนวณมูลค่าการไปย้อนหลังจากปลายไปยังต้น ซึ่งสุดท้ายทำให้ได้แผนการเดินทางที่มีค่าใช้จ่ายต่ำสุด \n",
    "โดยจากตัวอย่างนี้พบว่าเส้นทางเหมาะที่สุดไม่เป็นหนึ่งเดียว แต่มีสองคำตอบคือ ABDCA และ ACDBA ที่จะมีค่าใช้จ่ายเท่ากันคือ 17  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## บรรณานุกรม\n",
    "\n",
    "1. R.E. Bellman. Dynamic Programming. Princeton University Press. 1957.\n",
    "\n",
    "2. D.P. Bertsekas. Reinforcement Learning and Optimal Control. MIT Press. 2019.\n",
    "\n",
    "3. D.P. Bertsekas. [A Course in Reinforcement Learning, 2nd ed](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://web.mit.edu/dimitrib/www/RLCOURSECOMPLETE%202ndEDITION.pdf). Athena Scientific. 2025. \n",
    "\n",
    "4. Z. Manchester et.al. [16-745 Optimal Control & Reinforcement Learning, \n",
    "Course materials](https://optimalcontrol.ri.cmu.edu/#learning-resources), Carnegie Mellon University. 2024,2025.\n",
    "\n",
    "5. R. Tedrake. [Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and Manipulation (Course Notes for MIT 6.832)](https://underactuated.csail.mit.edu). 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dewdotninja/sharing-github/refs/heads/master/dewninja_logo50.jpg\" alt=\"dewninja\"/>\n",
    "</div>\n",
    "<div align=\"center\">dew.ninja 2025</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (controlenv)",
   "language": "python",
   "name": "controlennv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
